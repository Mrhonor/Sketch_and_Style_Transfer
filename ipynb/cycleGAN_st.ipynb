{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'options'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-865c5e8c3744>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTrainOptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCreate_nets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGet_dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'options'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from options import TrainOptions\n",
    "from models import Create_nets\n",
    "from datasets import Get_dataloader\n",
    "\n",
    "from optimizer import Get_loss_func, Get_optimizers\n",
    "from utils import ReplayBuffer, LambdaLR, sample_images\n",
    "\n",
    "#load the args\n",
    "args = TrainOptions().parse()\n",
    "# Calculate output of size discriminator (PatchGAN)\n",
    "patch = (1, args.img_height//(2**args.n_D_layers) - 2 , args.img_width//(2**args.n_D_layers) - 2)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G__AB, D__B, G__BA, D__A = Create_nets(args)\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN, criterion_cycle, criterion_identity = Get_loss_func(args)\n",
    "# Optimizers\n",
    "optimizer_G, optimizer_D_B, optimizer_D_A = Get_optimizers(args, G__AB, G__BA, D__B, D__A )\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(args.epoch_num, args.epoch_start, args.decay_epoch).step)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(args.epoch_num, args.epoch_start, args.decay_epoch).step)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(args.epoch_num, args.epoch_start, args.decay_epoch).step)\n",
    "\n",
    "# Configure dataloaders\n",
    "train_dataloader,test_dataloader,_ = Get_dataloader(args)\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_Y_A_buffer = ReplayBuffer()\n",
    "fake_X_B_buffer = ReplayBuffer()\n",
    "\n",
    "\n",
    "#  Training\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(args.epoch_start, args.epoch_num):\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Set model input\n",
    "        real_X_A = Variable(batch['X'].type(torch.FloatTensor).cuda())\n",
    "        real_Y_B = Variable(batch['Y'].type(torch.FloatTensor).cuda())\n",
    "\n",
    "         # Adversarial ground truths\n",
    "        valid = Variable(torch.FloatTensor(np.ones((real_X_A.size(0), *patch))).cuda(), requires_grad=False)\n",
    "        fake = Variable(torch.FloatTensor(np.zeros((real_X_A.size(0), *patch))).cuda(), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        # Identity loss\n",
    "        loss_id_A = criterion_identity(G__BA(real_X_A), real_X_A)\n",
    "        loss_id_B = criterion_identity(G__AB(real_Y_B), real_Y_B)\n",
    "\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "        # GAN loss\n",
    "        fake_X_B = G__AB(real_X_A)\n",
    "        pred_fake = D__B(fake_X_B)\n",
    "        #print(pred_fake.shape,valid.shape)\n",
    "        loss_GAN_AB = criterion_GAN(pred_fake, valid)\n",
    "\n",
    "        fake_Y_A = G__BA(real_Y_B)\n",
    "        pred_fake = D__A(fake_Y_A)\n",
    "        loss_GAN_BA = criterion_GAN(pred_fake, valid)\n",
    "\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "        # Cycle loss\n",
    "        recov_X_A = G__BA(fake_X_B)\n",
    "        loss_cycle_A = criterion_cycle(recov_X_A, real_X_A)\n",
    "        recov_Y_B = G__AB(fake_Y_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_Y_B, real_Y_B)\n",
    "\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Total loss\n",
    "        loss_G =    loss_GAN + \\\n",
    "                    args.lambda_cyc * loss_cycle + \\\n",
    "                    args.lambda_id * loss_identity\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        #  Train Discriminator A\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = D__A(real_X_A)\n",
    "        loss_real = criterion_GAN(pred_real, valid)\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        fake_Y_A_ = fake_Y_A_buffer.push_and_pop(fake_Y_A)\n",
    "        pred_fake = D__A(fake_Y_A_.detach())\n",
    "        loss_fake = criterion_GAN(pred_fake, fake)\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "        \n",
    "        #  Train Discriminator B\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = D__B(real_Y_B)\n",
    "        loss_real = criterion_GAN(pred_real, valid)\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        fake_X_B_ = fake_X_B_buffer.push_and_pop(fake_X_B)\n",
    "        pred_fake = D__B(fake_X_B_.detach())\n",
    "        loss_fake = criterion_GAN(pred_fake, fake)\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "        #  Log Progress\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(train_dataloader) + i\n",
    "        batches_left = args.epoch_num * len(train_dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\" %\n",
    "                                                        (epoch+1, args.epoch_num,\n",
    "                                                        i, len(train_dataloader),\n",
    "                                                        loss_D.data.cpu(), loss_G.data.cpu(),\n",
    "                                                        loss_GAN.data.cpu(), loss_cycle.data.cpu(),\n",
    "                                                        loss_identity.data.cpu(), time_left))\n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % args.sample_interval == 0:\n",
    "            sample_images(args,G__AB,G__BA, test_dataloader, epoch, batches_done)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step(epoch)\n",
    "    lr_scheduler_D_B.step(epoch)\n",
    "    lr_scheduler_D_A.step(epoch)\n",
    "\n",
    "    if args.checkpoint_interval != -1 and epoch % args.checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save(G__AB.state_dict(), '%s/%s/G__AB_%d.pth' % (args.model_result_dir, args.dataset_name, epoch))\n",
    "        torch.save(G__BA.state_dict(), '%s/%s/G__BA_%d.pth' % (args.model_result_dir, args.dataset_name, epoch))\n",
    "        torch.save(D_A.state_dict(), '%s/%s/D__A_%d.pth' % (args.model_result_dir, args.dataset_name, epoch))\n",
    "        torch.save(D_B.state_dict(), '%s/%s/D__B_%d.pth' % (args.model_result_dir, args.dataset_name, epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
